{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087dfebc-e737-4e46-b4e2-3c1963d4109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import kaggle as kg\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07633c49-0fa6-4629-9213-9a0476b7cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Successful\n"
     ]
    }
   ],
   "source": [
    "# Importando o dataset\n",
    "dataset_handle = \"abdelghaniaaba/wildfire-prediction-dataset\"\n",
    "destination_path = \"./datasets/wildfire-prediction\"\n",
    "\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# Comentar bloco abaixo em reexecução\n",
    "\n",
    "#print(f\"Downloading dataset to '{destination_path}'...\")\n",
    "\n",
    "#kaggle_command = f'kaggle datasets download -d {dataset_handle} -p \"{destination_path}\" --unzip'\n",
    "#print(f'Executing {kaggle_command}\\n')\n",
    "\n",
    "#os.system(kaggle_command)\n",
    "print(\"Download Successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0c9177-e347-4346-baa1-f7b06f1b6a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unspliting the dataset in './datasets/wildfire-prediction'...\n",
      "Folders created in './datasets/unsp_wildfire-prediction'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando diretórios\n",
    "unsplit_dataset = \"./datasets/unsp_wildfire-prediction\"\n",
    "print(f\"Unspliting the dataset in '{destination_path}'...\")\n",
    "\n",
    "final_wildfire_path = os.path.join(unsplit_dataset, 'wildfire')\n",
    "final_no_wildfire_path = os.path.join(unsplit_dataset, 'nowildfire')\n",
    "\n",
    "os.makedirs(final_wildfire_path, exist_ok=True)\n",
    "os.makedirs(final_no_wildfire_path, exist_ok=True)\n",
    "\n",
    "print(f\"Folders created in '{unsplit_dataset}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85916480-0a0b-488f-ba9f-365d998757bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unspliting the dataset in './datasets/wildfire-prediction'...\n",
      "Folders created in './datasets/unsp_wildfire-prediction'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving from train/wildfire: 100%|██████████████████████████████████████████████| 15750/15750 [00:04<00:00, 3250.90it/s]\n",
      "Moving from train/nowildfire: 100%|████████████████████████████████████████████| 14500/14500 [00:04<00:00, 3206.65it/s]\n",
      "Moving from test/wildfire: 100%|█████████████████████████████████████████████████| 3480/3480 [00:01<00:00, 3265.16it/s]\n",
      "Moving from test/nowildfire: 100%|███████████████████████████████████████████████| 2820/2820 [00:00<00:00, 3209.79it/s]\n",
      "Moving from valid/wildfire: 100%|████████████████████████████████████████████████| 3480/3480 [00:01<00:00, 3267.54it/s]\n",
      "Moving from valid/nowildfire: 100%|██████████████████████████████████████████████| 2820/2820 [00:00<00:00, 3220.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process done! Total of 42850 files moved.\n",
      "Unsplited dataset is at './datasets/unsp_wildfire-prediction'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_files_moved = 0\n",
    "split_folders = ['train', 'test', 'valid']\n",
    "\n",
    "for split in split_folders:\n",
    "    for class_name in ['wildfire', 'nowildfire']:\n",
    "        source_folder = os.path.join(destination_path, split, class_name)\n",
    "        destination_folder = os.path.join(unsplit_dataset, class_name)\n",
    "\n",
    "        if not os.path.isdir(source_folder):\n",
    "            print(f\"Warning: Source folder not found, skipping: {source_folder}\")\n",
    "            continue\n",
    "\n",
    "        files = os.listdir(source_folder)\n",
    "\n",
    "        for file_name in tqdm(files, desc=f\"Moving from {split}/{class_name}\"):\n",
    "            new_file_name = f\"{split}_{file_name}\"\n",
    "\n",
    "            source_file_path = os.path.join(source_folder, file_name)\n",
    "            destination_file_path = os.path.join(destination_folder, new_file_name)\n",
    "\n",
    "            shutil.move(source_file_path, destination_file_path)\n",
    "            total_files_moved += 1\n",
    "\n",
    "print(f\"Process done! Total of {total_files_moved} files moved.\")\n",
    "print(f\"Unsplited dataset is at '{unsplit_dataset}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844e2d22-b1b1-45ee-9974-59baca1a9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indo para o processo de extração de características\n",
    "from desc_imagens.lbp import lbp\n",
    "import cv2\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "wildfire_files = os.listdir(final_wildfire_path)\n",
    "no_wildfire_files = os.listdir(final_no_wildfire_path)\n",
    "output_folder = './output/output_features'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "features_file = os.path.join(output_folder, 'features_lbp.npy')\n",
    "labels_file = os.path.join(output_folder, 'labels_lbp.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9a9199e-cb68-4701-82f7-e32956194148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_path, label):\n",
    "    image_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image_gray is not None:\n",
    "        feature_vector = lbp(image_gray)\n",
    "        return feature_vector, label\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa62c8a3-cf13-4942-ae98-d9812a7711ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing processing task list...\n",
      "Total of 42850 images to process.\n",
      "Starting feature extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9895c5c9244db1bb23f723c02f7cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spliting features and labels...\n",
      "Converting to numpy array and saving...\n",
      "Features array dimension (X): (42850, 59)\n",
      "Labels array dimension (y): (42850,)\n",
      "Saving the arrays in './output/output_features'...\n",
      "Files saved: \n",
      "- ./output/output_features\\features_lbp.npy\n",
      "- ./output/output_features\\labels_lbp.npy\n"
     ]
    }
   ],
   "source": [
    "# Extraindo características usando LBP e \n",
    "# salvando as features e as labels referentes em './output/output_features'\n",
    "\n",
    "tasks = []\n",
    "classes = {\n",
    "    'wildfire': 1,\n",
    "    'nowildfire': 0\n",
    "}\n",
    "\n",
    "\n",
    "print(f'Preparing processing task list...')\n",
    "for class_name, label  in classes.items():\n",
    "    class_path = os.path.join(unsplit_dataset, class_name)\n",
    "    image_files = os.listdir(class_path)\n",
    "    for file_name in image_files:\n",
    "        tasks.append((os.path.join(class_path, file_name), label))\n",
    "\n",
    "print(f\"Total of {len(tasks)} images to process.\")\n",
    "\n",
    "print(\"Starting feature extraction...\")\n",
    "\n",
    "# Usando todos os núcleos do CPU\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_image)(path, lbl) for path, lbl in tqdm(tasks) \n",
    ")\n",
    "\n",
    "print(\"\\nSpliting features and labels...\")\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for feature, label in results:\n",
    "    if feature is not None:\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "\n",
    "# Convertendo as listas em numpy arrays\n",
    "print(\"Converting to numpy array and saving...\")\n",
    "\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f'Features array dimension (X): {X.shape}')\n",
    "print(f'Labels array dimension (y): {y.shape}')\n",
    "\n",
    "print(f\"Saving the arrays in '{output_folder}'...\")\n",
    "np.save(features_file, X)\n",
    "np.save(labels_file, y)\n",
    "print(f\"Files saved: \\n- {features_file}\\n- {labels_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d362825-bb21-4feb-b7e9-4ee580bf7cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features array dimension (X): (42850, 59)\n",
      "Labels array dimension (y): (42850,)\n"
     ]
    }
   ],
   "source": [
    "# Lendo arrays salvas nas outputs\n",
    "print('Loading features and labels...')\n",
    "\n",
    "X = np.load(features_file)\n",
    "print(f'Features array dimension (X): {X.shape}')\n",
    "y = np.load(labels_file)\n",
    "print(f'Labels array dimension (y): {y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e6813-ea2a-42ab-8091-14e35c53758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Separando conjunto de teste e treino (80% treino, 20% teste)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(f'Data split into {len(X_train)} train samples and {len(X_test)} test samples.')\n",
    "\n",
    "# Começando processo de otimização e avalização por validação cruzada\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
